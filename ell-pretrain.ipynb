{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# SETUP","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer\nfrom transformers import BertTokenizer, TFBertModel\nimport re\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nnltk.download('punkt')\n!pip install contractions\nimport contractions\nimport copy \nnltk.download('stopwords')\nnltk.download('omw-1.4')\nfrom textblob import TextBlob\nimport spacy\nfrom spacy import displacy\nfrom tensorflow.keras.utils import plot_model\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tqdm import tqdm\ntqdm.pandas()\n\nfrom lightgbm import LGBMRegressor\nfrom sklearn import model_selection\nfrom sklearn import metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_trial = pd.read_csv('../input/feedback-prize-english-language-learning/train.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''df_trial = df_train.head(10)\ndf_trial'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# HELPER FUNCTIONS","metadata":{}},{"cell_type":"code","source":"def clean(df, t_ids):\n    clean_text = []\n    \n    for t_id in tqdm(t_ids):\n        temp_df = df.loc[df['text_id']==t_id]\n        \n        text = temp_df['full_text'].to_numpy()\n        #print(text)\n    \n        \n        text = text[0].lower()\n        text = re.sub(\"[^\\w\\s]\", \" \", text)\n        \n        text = text.split()\n        \n        \n        text_nostop = []\n        for word in text:\n            if word not in stopwords.words():\n                text_nostop.append(word)\n                \n        lemmatizer = WordNetLemmatizer()\n        text_lem = []\n        for word in text_nostop:\n            text_lem.append(lemmatizer.lemmatize(word))\n            \n        text = \" \".join(text_lem)\n        clean_text.append(text)\n        \n        # break\n        \n    #print(clean_text)\n    df['clean_text'] = clean_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t_ids = df_trial['text_id'].to_numpy()\nclean(df_trial, t_ids)\ndf_trial.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_trial.to_csv('ELL-clean.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}