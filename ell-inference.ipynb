{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# SETUP","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer\nfrom transformers import BertTokenizer, TFBertModel\nimport re\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus import stopwords\n#nltk.download('punkt')\n#!pip install contractions\nimport copy \n#nltk.download('stopwords')\n#nltk.download('omw-1.4')\nfrom textblob import TextBlob\nimport spacy\nfrom spacy import displacy\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import regularizers\n\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\ntqdm.pandas()\n\nfrom sklearn import model_selection\nfrom sklearn import metrics","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-14T05:09:13.994334Z","iopub.execute_input":"2022-11-14T05:09:13.994629Z","iopub.status.idle":"2022-11-14T05:09:50.096376Z","shell.execute_reply.started":"2022-11-14T05:09:13.994566Z","shell.execute_reply":"2022-11-14T05:09:50.095427Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2022-11-14 05:09:29.997976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 05:09:29.999108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 05:09:30.000172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 05:09:30.000953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 05:09:30.001714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 05:09:30.002537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 05:09:30.006705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-11-14 05:09:30.260086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 05:09:30.260991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 05:09:30.261793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 05:09:30.262534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 05:09:30.263212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 05:09:30.263931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 05:09:41.197854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 05:09:41.198768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 05:09:41.199526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 05:09:41.200259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 05:09:41.201003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 05:09:41.201688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13351 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n2022-11-14 05:09:41.207663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 05:09:41.208395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13351 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"}]},{"cell_type":"code","source":"BASE_MODEL = '../input/huggingface-bert-variants/bert-base-cased/bert-base-cased'\ntokenizer = BertTokenizer.from_pretrained(BASE_MODEL)\nnlp = spacy.load(\"en_core_web_sm\")\nmaxlen = 100\nmaxsent = 10","metadata":{"execution":{"iopub.status.busy":"2022-11-14T05:09:50.098467Z","iopub.execute_input":"2022-11-14T05:09:50.099560Z","iopub.status.idle":"2022-11-14T05:09:51.164203Z","shell.execute_reply.started":"2022-11-14T05:09:50.099517Z","shell.execute_reply":"2022-11-14T05:09:51.163284Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df_trial = pd.read_csv('../input/feedback-prize-english-language-learning/test.csv')\ndf_trial.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-14T05:09:51.168289Z","iopub.execute_input":"2022-11-14T05:09:51.168586Z","iopub.status.idle":"2022-11-14T05:09:51.205078Z","shell.execute_reply.started":"2022-11-14T05:09:51.168558Z","shell.execute_reply":"2022-11-14T05:09:51.204269Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"        text_id                                          full_text\n0  0000C359D63E  when a person has no experience on a job their...\n1  000BAD50D026  Do you think students would benefit from being...\n2  00367BB2546B  Thomas Jefferson once states that \"it is wonde...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>full_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000C359D63E</td>\n      <td>when a person has no experience on a job their...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000BAD50D026</td>\n      <td>Do you think students would benefit from being...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00367BB2546B</td>\n      <td>Thomas Jefferson once states that \"it is wonde...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# HELPER FUNCTIONS","metadata":{}},{"cell_type":"code","source":"def clean(df, t_ids):\n    clean_text = []\n    nlp = spacy.load(\"en_core_web_sm\")\n    \n    for t_id in tqdm(t_ids):\n        temp_df = df.loc[df['text_id']==t_id]\n        \n        text = temp_df['full_text'].to_numpy()\n        #print(text)\n    \n        \n        text = text[0].lower()\n        text = re.sub(\"[^\\w\\s]\", \" \", text)\n        \n        '''text = text.split()\n        \n        \n        text_nostop = []\n        for word in text:\n            if word not in stopwords.words():\n                text_nostop.append(word)\n                \n        text_lem = []\n        for word in text_nostop:\n            word = nlp(word)\n            text_lem.append(word.lemma_)\n            \n        text = \" \".join(text_lem)'''\n        clean_text.append(text)\n        \n        # break\n        \n    #print(clean_text)\n    df['clean_text'] = clean_text","metadata":{"execution":{"iopub.status.busy":"2022-11-14T05:09:51.210354Z","iopub.execute_input":"2022-11-14T05:09:51.211194Z","iopub.status.idle":"2022-11-14T05:09:51.220611Z","shell.execute_reply.started":"2022-11-14T05:09:51.211156Z","shell.execute_reply":"2022-11-14T05:09:51.219682Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def equalize_lists(lst, maxlen):\n    length = len(lst)\n    dif = abs(maxlen - length)\n    \n    if maxlen > length and dif != 0:\n        lst += [0] * dif\n        \n    elif maxlen < length and dif > 0:\n        lst = lst[:len(lst)-dif]\n        \n    #print(lst)\n    return lst","metadata":{"execution":{"iopub.status.busy":"2022-11-14T05:09:51.222286Z","iopub.execute_input":"2022-11-14T05:09:51.223370Z","iopub.status.idle":"2022-11-14T05:09:51.233867Z","shell.execute_reply.started":"2022-11-14T05:09:51.223331Z","shell.execute_reply":"2022-11-14T05:09:51.232982Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def vocab_dictionary(df, t_ids):\n    vocab_dict = {}\n    \n    for t_id in tqdm(t_ids):\n        temp_df = df.loc[df['text_id']==t_id]\n        temp_dict = {}\n        \n        text = temp_df['clean_text'].to_numpy()\n        words = text[0].split()\n        #print(words)\n        \n        for word in words:\n            if word not in temp_dict.keys():\n                temp_dict[word] = 1\n            \n            else:\n                temp_dict[word] += 1\n         \n        #print()\n        #print(temp_dict)\n        \n        vocab_dict[t_id] = temp_dict\n        #print(vocab_dict)\n        \n        #break\n        \n    return vocab_dict","metadata":{"execution":{"iopub.status.busy":"2022-11-14T05:09:51.237868Z","iopub.execute_input":"2022-11-14T05:09:51.239324Z","iopub.status.idle":"2022-11-14T05:09:51.248416Z","shell.execute_reply.started":"2022-11-14T05:09:51.239284Z","shell.execute_reply":"2022-11-14T05:09:51.247314Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def frequency_ratio(vocab_dict, t_ids, maxlen):\n    #print(vocab_dict)\n    out = []\n    \n    for t_id in tqdm(t_ids):\n        vals = vocab_dict[t_id]\n        #print(vals)\n        freq = list(vals.values())\n        #print(freq)\n        max_use = max(freq)\n        #print(max_use)\n        \n        temp = [val/max_use for val in freq]\n        temp = equalize_lists(temp, maxlen)\n        temp = np.asarray(temp)\n        #print(freq)\n        #print(temp)\n        out.append(temp)\n        \n        #break \n        \n    #print(out)\n    return out ","metadata":{"execution":{"iopub.status.busy":"2022-11-14T05:09:51.250029Z","iopub.execute_input":"2022-11-14T05:09:51.250669Z","iopub.status.idle":"2022-11-14T05:09:51.260478Z","shell.execute_reply.started":"2022-11-14T05:09:51.250633Z","shell.execute_reply":"2022-11-14T05:09:51.259131Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def get_processing_hash(df, t_ids, feature, maxlen):\n    out = []\n    \n    all_text = df['full_text'].to_list()\n    \n    for i, text in enumerate(tqdm(all_text)):\n        #print(i)\n        temp_out = []\n        #print(text)\n        doc = nlp(text)\n        \n        for token in doc:\n            #print(token, token.pos_)\n            try: \n                if feature == 'pos':\n                    temp_out.append(token.pos)\n\n                elif feature == 'tag':\n                    temp_out.append(token.tag)\n\n                else:\n                    temp_out.append(token.dep)\n                    \n            except Exception:\n                pass\n                \n        #print(temp_out)\n        length = len(temp_out)\n        temp_out = equalize_lists(temp_out, maxlen)\n            \n        out.append(temp_out)\n        #break \n        \n    #print(out)\n    return out","metadata":{"execution":{"iopub.status.busy":"2022-11-14T05:09:51.262172Z","iopub.execute_input":"2022-11-14T05:09:51.262861Z","iopub.status.idle":"2022-11-14T05:09:51.272540Z","shell.execute_reply.started":"2022-11-14T05:09:51.262827Z","shell.execute_reply":"2022-11-14T05:09:51.271441Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def tokenize_text(df, t_ids, tokenizer, maxlen, pairwise=False):\n    input_ids = {}\n    att_mask = {}\n    token_ids = {}\n    \n    '''if clean == False: \n        use = 'full_text'\n    else:\n        use = 'clean_text'''\n    \n    for t_id in tqdm(t_ids):\n        temp_df = df.loc[df['text_id']==t_id]\n        \n        temp_input_ids = []\n        temp_att_mask = []\n        temp_token_ids = []\n        \n        text = temp_df['full_text'].to_numpy() # THIS USED TO BE USE \n        #print(text)\n        #sentences = text[0].split()\n        sentences = nltk.tokenize.sent_tokenize(text[0])\n        #print(sentences)\n        \n        if pairwise == False:\n            #for sentence in sentences:\n            encoded = tokenizer.batch_encode_plus(text, add_special_tokens=True, return_token_type_ids=True, truncation=True, \n                                                 padding='max_length', max_length=maxlen)\n\n            temp_input_ids.append(encoded['input_ids'])\n            temp_att_mask.append(encoded['attention_mask'])\n            temp_token_ids.append(encoded['token_type_ids'])\n            \n            #print(encoded['input_ids'])\n            #print(encoded['token_type_ids'])\n                \n        else:\n            #print(\"pointwise\")\n            #print(sentences)\n            for i in range(len(sentences)-1):\n                #print(i)\n                pair = sentences[i:i+2]\n                #print(pair)\n                \n                encoded = tokenizer.encode_plus(pair[0], pair[1], add_special_tokens=True, return_token_type_ids=True, truncation=False,\n                                                padding='max_length', max_length=maxlen)\n                \n                temp_input_ids.append(encoded['input_ids'])\n                temp_att_mask.append(encoded['attention_mask'])\n                temp_token_ids.append(encoded['token_type_ids'])\n        \n        input_ids[t_id] = temp_input_ids\n        att_mask[t_id] = temp_att_mask\n        token_ids[t_id] = temp_token_ids\n        \n        #print(input_ids)\n        \n                \n    #print(input_ids)\n    return input_ids, att_mask, token_ids","metadata":{"execution":{"iopub.status.busy":"2022-11-14T05:09:51.274035Z","iopub.execute_input":"2022-11-14T05:09:51.274600Z","iopub.status.idle":"2022-11-14T05:09:51.291578Z","shell.execute_reply.started":"2022-11-14T05:09:51.274567Z","shell.execute_reply":"2022-11-14T05:09:51.290554Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def jaccard(list1, list2):\n    intersection = len(list(set(list1).intersection(list2)))\n    union = (len(list1) + len(list2)) - intersection\n    return float(intersection) / union","metadata":{"execution":{"iopub.status.busy":"2022-11-14T05:09:51.299009Z","iopub.execute_input":"2022-11-14T05:09:51.299414Z","iopub.status.idle":"2022-11-14T05:09:51.315061Z","shell.execute_reply.started":"2022-11-14T05:09:51.299381Z","shell.execute_reply":"2022-11-14T05:09:51.313877Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def similarity(pair):\n    split_id = pair.index(102)\n    #print(split_id)\n    \n    pair_1 = pair[1:split_id]\n    pair_2 = pair[split_id+1:-1]\n    #print(pair_1) \n    #print(pair_2)\n    \n    pair_similarity = jaccard(pair_1, pair_2)\n    #print(pair_similarity)\n    \n    return(pair_similarity)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T05:09:51.316296Z","iopub.execute_input":"2022-11-14T05:09:51.316642Z","iopub.status.idle":"2022-11-14T05:09:51.330626Z","shell.execute_reply.started":"2022-11-14T05:09:51.316611Z","shell.execute_reply":"2022-11-14T05:09:51.328961Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# DATA SETUP FOR INFERENCE ","metadata":{}},{"cell_type":"code","source":"t_ids = df_trial['text_id'].to_numpy()\nclean(df_trial, t_ids)\ndf_trial.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-14T05:09:51.332061Z","iopub.execute_input":"2022-11-14T05:09:51.332466Z","iopub.status.idle":"2022-11-14T05:09:51.957177Z","shell.execute_reply.started":"2022-11-14T05:09:51.332419Z","shell.execute_reply":"2022-11-14T05:09:51.956131Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"100%|██████████| 3/3 [00:00<00:00, 539.65it/s]\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"        text_id                                          full_text  \\\n0  0000C359D63E  when a person has no experience on a job their...   \n1  000BAD50D026  Do you think students would benefit from being...   \n2  00367BB2546B  Thomas Jefferson once states that \"it is wonde...   \n\n                                          clean_text  \n0  when a person has no experience on a job their...  \n1  do you think students would benefit from being...  \n2  thomas jefferson once states that  it is wonde...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>full_text</th>\n      <th>clean_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000C359D63E</td>\n      <td>when a person has no experience on a job their...</td>\n      <td>when a person has no experience on a job their...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000BAD50D026</td>\n      <td>Do you think students would benefit from being...</td>\n      <td>do you think students would benefit from being...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00367BB2546B</td>\n      <td>Thomas Jefferson once states that \"it is wonde...</td>\n      <td>thomas jefferson once states that  it is wonde...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"v_dict = vocab_dictionary(df_trial, t_ids)\nfreq_ratio = frequency_ratio(v_dict, t_ids, maxlen)\n\npos_inputs = get_processing_hash(df_trial, t_ids, 'pos', maxlen)\ntag_inputs = get_processing_hash(df_trial, t_ids, 'tag', maxlen)\ndep_inputs = get_processing_hash(df_trial, t_ids, 'dep', maxlen)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T05:09:51.958646Z","iopub.execute_input":"2022-11-14T05:09:51.959679Z","iopub.status.idle":"2022-11-14T05:09:52.767735Z","shell.execute_reply.started":"2022-11-14T05:09:51.959640Z","shell.execute_reply":"2022-11-14T05:09:52.766658Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"100%|██████████| 3/3 [00:00<00:00, 910.29it/s]\n100%|██████████| 3/3 [00:00<00:00, 7776.83it/s]\n100%|██████████| 3/3 [00:00<00:00,  9.87it/s]\n100%|██████████| 3/3 [00:00<00:00, 12.62it/s]\n100%|██████████| 3/3 [00:00<00:00, 12.79it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"unclean_pairs_ids, unclean_pairs_masks, unclean_pairs_tokens = tokenize_text(df_trial, t_ids, tokenizer, maxlen, True)\nbatch_ids, batch_masks, batch_tokens = tokenize_text(df_trial, t_ids, tokenizer, maxlen, False)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T05:09:52.769463Z","iopub.execute_input":"2022-11-14T05:09:52.770176Z","iopub.status.idle":"2022-11-14T05:09:52.930950Z","shell.execute_reply.started":"2022-11-14T05:09:52.770137Z","shell.execute_reply":"2022-11-14T05:09:52.929829Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"100%|██████████| 3/3 [00:00<00:00, 28.61it/s]\n100%|██████████| 3/3 [00:00<00:00, 76.60it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"unclean_similarity = []\n\nfor t_id in tqdm(t_ids):\n    #print(unclean_pairs_ids[t_id])\n    temp_sim = []\n    i = 0\n    for pair in unclean_pairs_ids[t_id]:\n        if i < maxsent: \n            #print(pair)\n            temp_sim.append(similarity(pair))\n            i += 1\n        else:\n            break\n    \n    temp_sim = equalize_lists(temp_sim, maxsent)\n    unclean_similarity.append(temp_sim)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T05:09:52.932758Z","iopub.execute_input":"2022-11-14T05:09:52.933429Z","iopub.status.idle":"2022-11-14T05:09:52.948409Z","shell.execute_reply.started":"2022-11-14T05:09:52.933388Z","shell.execute_reply":"2022-11-14T05:09:52.947114Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"100%|██████████| 3/3 [00:00<00:00, 7100.97it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_ids, batch_masks, batch_tokens = tokenize_text(df_trial, t_ids, tokenizer, maxlen, False)\n\nbert_ids = []\nbert_masks = []\nbert_tokens = []\n\nfor t_id in t_ids:\n    bert_ids.append((batch_ids[t_id][0][0]))\n    bert_masks.append((batch_masks[t_id][0][0]))\n    bert_tokens.append((batch_tokens[t_id][0][0]))\n    \n    #break","metadata":{"execution":{"iopub.status.busy":"2022-11-14T05:09:52.949761Z","iopub.execute_input":"2022-11-14T05:09:52.950291Z","iopub.status.idle":"2022-11-14T05:09:53.003598Z","shell.execute_reply.started":"2022-11-14T05:09:52.950251Z","shell.execute_reply":"2022-11-14T05:09:53.002293Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"100%|██████████| 3/3 [00:00<00:00, 72.06it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"bert_ids_np = np.asarray(bert_ids, dtype=np.float64)\nbert_masks_np = np.asarray(bert_masks, dtype=np.float64)\nbert_tokens_np = np.asarray(bert_tokens, dtype=np.float64)\n\nfreq_ratio_np = np.asarray(freq_ratio)\npos_inputs_np = np.asarray(pos_inputs)\ntag_inputs_np = np.asarray(tag_inputs)\ndep_inputs_np = np.asarray(dep_inputs)\nunclean_similarity_np = np.asarray(unclean_similarity)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T05:09:53.005261Z","iopub.execute_input":"2022-11-14T05:09:53.005706Z","iopub.status.idle":"2022-11-14T05:09:53.013900Z","shell.execute_reply.started":"2022-11-14T05:09:53.005668Z","shell.execute_reply":"2022-11-14T05:09:53.012821Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# MODEL SETUP FOR INFERENCE","metadata":{}},{"cell_type":"code","source":"def build_reg_model():\n    backbone = TFBertModel.from_pretrained(BASE_MODEL)\n    \n    input_ids = layers.Input(\n        shape=(maxlen,),\n        dtype=tf.int32,\n        name=\"input_ids\",\n    )\n    \n    attention_mask = layers.Input(\n        shape=(maxlen,),\n        dtype=tf.int32,\n        name=\"attention_mask\",\n    )\n    \n    token_type_ids = layers.Input(\n        shape=(maxlen,),\n        dtype=tf.int32,\n        name=\"token_type_ids\",\n    )\n    \n    bert_out = backbone({\n            \"input_ids\": input_ids,\n            \"token_type_ids\": token_type_ids,\n            \"attention_mask\": attention_mask})\n    \n    similarity = tf.keras.layers.Input((maxsent,), dtype=tf.float32, name='similarity') \n    \n    freq_ratio = tf.keras.layers.Input((maxlen,), dtype=tf.float32, name='freq_ratio') \n    \n    pos = tf.keras.layers.Input((maxlen,), dtype=tf.float32, name='pos') \n    tag = tf.keras.layers.Input((maxlen,), dtype=tf.float32, name='tag') \n    dep = tf.keras.layers.Input((maxlen,), dtype=tf.float32, name='dep') \n    \n    bert_out = tf.keras.layers.Dense(64, activation=\"linear\", dtype=\"float32\")(bert_out[0][:, 0, :])\n    concat = tf.keras.layers.Concatenate()([similarity, freq_ratio, pos, tag, dep, bert_out])\n    \n    x = tf.keras.layers.LayerNormalization()(concat)\n    x = tf.keras.layers.Dense(32)(x)\n    x = tf.keras.layers.LayerNormalization()(x)\n    x = tf.keras.layers.Dense(16)(x)\n    x = tf.keras.layers.LayerNormalization()(x)\n    \n    out = tf.keras.layers.Dense(6, dtype='float32')(x)\n    \n    model = tf.keras.Model(\n        inputs=[input_ids, attention_mask, token_type_ids, similarity, freq_ratio, pos, tag, dep],\n        outputs=[out],\n    )\n    \n    \n    model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n    loss=tf.keras.losses.MeanSquaredError()\n    )\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-11-14T05:09:53.015615Z","iopub.execute_input":"2022-11-14T05:09:53.016263Z","iopub.status.idle":"2022-11-14T05:09:53.033602Z","shell.execute_reply.started":"2022-11-14T05:09:53.016209Z","shell.execute_reply":"2022-11-14T05:09:53.032288Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"reloaded_reg_model = build_reg_model()\nreloaded_reg_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-11-14T05:09:53.035285Z","iopub.execute_input":"2022-11-14T05:09:53.035965Z","iopub.status.idle":"2022-11-14T05:10:14.560196Z","shell.execute_reply.started":"2022-11-14T05:09:53.035925Z","shell.execute_reply":"2022-11-14T05:10:14.559073Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"Some layers from the model checkpoint at ../input/huggingface-bert-variants/bert-base-cased/bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at ../input/huggingface-bert-variants/bert-base-cased/bert-base-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nattention_mask (InputLayer)     [(None, 100)]        0                                            \n__________________________________________________________________________________________________\ninput_ids (InputLayer)          [(None, 100)]        0                                            \n__________________________________________________________________________________________________\ntoken_type_ids (InputLayer)     [(None, 100)]        0                                            \n__________________________________________________________________________________________________\ntf_bert_model (TFBertModel)     TFBaseModelOutputWit 108310272   attention_mask[0][0]             \n                                                                 input_ids[0][0]                  \n                                                                 token_type_ids[0][0]             \n__________________________________________________________________________________________________\ntf.__operators__.getitem (Slici (None, 768)          0           tf_bert_model[0][0]              \n__________________________________________________________________________________________________\nsimilarity (InputLayer)         [(None, 10)]         0                                            \n__________________________________________________________________________________________________\nfreq_ratio (InputLayer)         [(None, 100)]        0                                            \n__________________________________________________________________________________________________\npos (InputLayer)                [(None, 100)]        0                                            \n__________________________________________________________________________________________________\ntag (InputLayer)                [(None, 100)]        0                                            \n__________________________________________________________________________________________________\ndep (InputLayer)                [(None, 100)]        0                                            \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 64)           49216       tf.__operators__.getitem[0][0]   \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 474)          0           similarity[0][0]                 \n                                                                 freq_ratio[0][0]                 \n                                                                 pos[0][0]                        \n                                                                 tag[0][0]                        \n                                                                 dep[0][0]                        \n                                                                 dense[0][0]                      \n__________________________________________________________________________________________________\nlayer_normalization (LayerNorma (None, 474)          948         concatenate[0][0]                \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 32)           15200       layer_normalization[0][0]        \n__________________________________________________________________________________________________\nlayer_normalization_1 (LayerNor (None, 32)           64          dense_1[0][0]                    \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 16)           528         layer_normalization_1[0][0]      \n__________________________________________________________________________________________________\nlayer_normalization_2 (LayerNor (None, 16)           32          dense_2[0][0]                    \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 6)            102         layer_normalization_2[0][0]      \n==================================================================================================\nTotal params: 108,376,362\nTrainable params: 108,376,362\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"reloaded_reg_model.load_weights('../input/ell-model-weights-callback/model_callback.h5')","metadata":{"execution":{"iopub.status.busy":"2022-11-14T05:10:14.561999Z","iopub.execute_input":"2022-11-14T05:10:14.562941Z","iopub.status.idle":"2022-11-14T05:10:18.182212Z","shell.execute_reply.started":"2022-11-14T05:10:14.562895Z","shell.execute_reply":"2022-11-14T05:10:18.181047Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"reg_preds = reloaded_reg_model.predict([bert_ids_np, bert_masks_np, bert_tokens_np, \n                                        unclean_similarity_np, freq_ratio_np, \n                                        pos_inputs_np, tag_inputs_np, dep_inputs_np])","metadata":{"execution":{"iopub.status.busy":"2022-11-14T05:10:18.183850Z","iopub.execute_input":"2022-11-14T05:10:18.184266Z","iopub.status.idle":"2022-11-14T05:10:23.225029Z","shell.execute_reply.started":"2022-11-14T05:10:18.184212Z","shell.execute_reply":"2022-11-14T05:10:23.223996Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"2022-11-14 05:10:18.287294: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n2022-11-14 05:10:22.846265: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n","output_type":"stream"}]},{"cell_type":"code","source":"length = len(t_ids)\n\ncohesion_preds = np.zeros(length)\nsyntax_preds = np.zeros(length)\nvocabulary_preds = np.zeros(length)\nphraseology_preds = np.zeros(length)\ngrammar_preds = np.zeros(length)\nconventions_preds = np.zeros(length)\n\nfor i in range(length):\n    cohesion_preds[i] = reg_preds[i][0]\n    syntax_preds[i] = reg_preds[i][1]\n    vocabulary_preds[i] = reg_preds[i][2]\n    phraseology_preds[i] = reg_preds[i][3]\n    grammar_preds[i] = reg_preds[i][4]\n    conventions_preds[i] = reg_preds[i][5]","metadata":{"execution":{"iopub.status.busy":"2022-11-14T05:10:23.226869Z","iopub.execute_input":"2022-11-14T05:10:23.227296Z","iopub.status.idle":"2022-11-14T05:10:23.235864Z","shell.execute_reply.started":"2022-11-14T05:10:23.227247Z","shell.execute_reply":"2022-11-14T05:10:23.234072Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"predictions = pd.DataFrame()\npredictions['text_id'] = t_ids\n\npredictions['cohesion'] = cohesion_preds\npredictions['syntax'] = syntax_preds\npredictions['vocabulary'] = vocabulary_preds\npredictions['phraseology'] = phraseology_preds\npredictions['grammar'] = grammar_preds\npredictions['conventions'] = conventions_preds\n\npredictions.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-14T05:10:23.237244Z","iopub.execute_input":"2022-11-14T05:10:23.237747Z","iopub.status.idle":"2022-11-14T05:10:23.271155Z","shell.execute_reply.started":"2022-11-14T05:10:23.237711Z","shell.execute_reply":"2022-11-14T05:10:23.270161Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"        text_id  cohesion    syntax  vocabulary  phraseology   grammar  \\\n0  0000C359D63E  3.142531  3.033324    3.215459     3.148757  3.008596   \n1  000BAD50D026  3.142531  3.033324    3.215459     3.148757  3.008596   \n2  00367BB2546B  3.142531  3.033324    3.215459     3.148757  3.008596   \n\n   conventions  \n0     3.070784  \n1     3.070784  \n2     3.070784  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000C359D63E</td>\n      <td>3.142531</td>\n      <td>3.033324</td>\n      <td>3.215459</td>\n      <td>3.148757</td>\n      <td>3.008596</td>\n      <td>3.070784</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000BAD50D026</td>\n      <td>3.142531</td>\n      <td>3.033324</td>\n      <td>3.215459</td>\n      <td>3.148757</td>\n      <td>3.008596</td>\n      <td>3.070784</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00367BB2546B</td>\n      <td>3.142531</td>\n      <td>3.033324</td>\n      <td>3.215459</td>\n      <td>3.148757</td>\n      <td>3.008596</td>\n      <td>3.070784</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"predictions.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T05:10:23.272954Z","iopub.execute_input":"2022-11-14T05:10:23.273426Z","iopub.status.idle":"2022-11-14T05:10:23.284298Z","shell.execute_reply.started":"2022-11-14T05:10:23.273383Z","shell.execute_reply":"2022-11-14T05:10:23.283245Z"},"trusted":true},"execution_count":24,"outputs":[]}]}