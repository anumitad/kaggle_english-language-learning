{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# SETUP","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer\nfrom transformers import BertTokenizer, TFBertModel\nimport re\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus import stopwords\n#nltk.download('punkt')\n#!pip install contractions\nimport copy \n#nltk.download('stopwords')\n#nltk.download('omw-1.4')\nfrom textblob import TextBlob\nimport spacy\nfrom spacy import displacy\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import regularizers\n\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\ntqdm.pandas()\n\nfrom sklearn import model_selection\nfrom sklearn import metrics","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BASE_MODEL = '../input/huggingface-bert-variants/bert-base-cased/bert-base-cased'\ntokenizer = BertTokenizer.from_pretrained(BASE_MODEL)\nnlp = spacy.load(\"en_core_web_sm\")\nmaxlen = 100\nmaxsent = 10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_trial = pd.read_csv('../input/feedback-prize-english-language-learning/test.csv')\ndf_trial.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# HELPER FUNCTIONS","metadata":{}},{"cell_type":"code","source":"def clean(df, t_ids):\n    clean_text = []\n    nlp = spacy.load(\"en_core_web_sm\")\n    \n    for t_id in tqdm(t_ids):\n        temp_df = df.loc[df['text_id']==t_id]\n        \n        text = temp_df['full_text'].to_numpy()\n        #print(text)\n    \n        \n        text = text[0].lower()\n        text = re.sub(\"[^\\w\\s]\", \" \", text)\n        \n        '''text = text.split()\n        \n        \n        text_nostop = []\n        for word in text:\n            if word not in stopwords.words():\n                text_nostop.append(word)\n                \n        text_lem = []\n        for word in text_nostop:\n            word = nlp(word)\n            text_lem.append(word.lemma_)\n            \n        text = \" \".join(text_lem)'''\n        clean_text.append(text)\n        \n        # break\n        \n    #print(clean_text)\n    df['clean_text'] = clean_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def equalize_lists(lst, maxlen):\n    length = len(lst)\n    dif = abs(maxlen - length)\n    \n    if maxlen > length and dif != 0:\n        lst += [0] * dif\n        \n    elif maxlen < length and dif > 0:\n        lst = lst[:len(lst)-dif]\n        \n    #print(lst)\n    return lst","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def vocab_dictionary(df, t_ids):\n    vocab_dict = {}\n    \n    for t_id in tqdm(t_ids):\n        temp_df = df.loc[df['text_id']==t_id]\n        temp_dict = {}\n        \n        text = temp_df['clean_text'].to_numpy()\n        words = text[0].split()\n        #print(words)\n        \n        for word in words:\n            if word not in temp_dict.keys():\n                temp_dict[word] = 1\n            \n            else:\n                temp_dict[word] += 1\n         \n        #print()\n        #print(temp_dict)\n        \n        vocab_dict[t_id] = temp_dict\n        #print(vocab_dict)\n        \n        #break\n        \n    return vocab_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def frequency_ratio(vocab_dict, t_ids, maxlen):\n    #print(vocab_dict)\n    out = []\n    \n    for t_id in tqdm(t_ids):\n        vals = vocab_dict[t_id]\n        #print(vals)\n        freq = list(vals.values())\n        #print(freq)\n        max_use = max(freq)\n        #print(max_use)\n        \n        temp = [val/max_use for val in freq]\n        temp = equalize_lists(temp, maxlen)\n        temp = np.asarray(temp)\n        #print(freq)\n        #print(temp)\n        out.append(temp)\n        \n        #break \n        \n    #print(out)\n    return out ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_processing_hash(df, t_ids, feature, maxlen):\n    out = []\n    \n    all_text = df['full_text'].to_list()\n    \n    for i, text in enumerate(tqdm(all_text)):\n        #print(i)\n        temp_out = []\n        #print(text)\n        doc = nlp(text)\n        \n        for token in doc:\n            #print(token, token.pos_)\n            try: \n                if feature == 'pos':\n                    temp_out.append(token.pos)\n\n                elif feature == 'tag':\n                    temp_out.append(token.tag)\n\n                else:\n                    temp_out.append(token.dep)\n                    \n            except Exception:\n                pass\n                \n        #print(temp_out)\n        length = len(temp_out)\n        temp_out = equalize_lists(temp_out, maxlen)\n            \n        out.append(temp_out)\n        #break \n        \n    #print(out)\n    return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_text(df, t_ids, tokenizer, maxlen, pairwise=False):\n    input_ids = {}\n    att_mask = {}\n    token_ids = {}\n    \n    '''if clean == False: \n        use = 'full_text'\n    else:\n        use = 'clean_text'''\n    \n    for t_id in tqdm(t_ids):\n        temp_df = df.loc[df['text_id']==t_id]\n        \n        temp_input_ids = []\n        temp_att_mask = []\n        temp_token_ids = []\n        \n        text = temp_df['full_text'].to_numpy() # THIS USED TO BE USE \n        #print(text)\n        #sentences = text[0].split()\n        sentences = nltk.tokenize.sent_tokenize(text[0])\n        #print(sentences)\n        \n        if pairwise == False:\n            #for sentence in sentences:\n            encoded = tokenizer.batch_encode_plus(text, add_special_tokens=True, return_token_type_ids=True, truncation=True, \n                                                 padding='max_length', max_length=maxlen)\n\n            temp_input_ids.append(encoded['input_ids'])\n            temp_att_mask.append(encoded['attention_mask'])\n            temp_token_ids.append(encoded['token_type_ids'])\n            \n            #print(encoded['input_ids'])\n            #print(encoded['token_type_ids'])\n                \n        else:\n            #print(\"pointwise\")\n            #print(sentences)\n            for i in range(len(sentences)-1):\n                #print(i)\n                pair = sentences[i:i+2]\n                #print(pair)\n                \n                encoded = tokenizer.encode_plus(pair[0], pair[1], add_special_tokens=True, return_token_type_ids=True, truncation=False,\n                                                padding='max_length', max_length=maxlen)\n                \n                temp_input_ids.append(encoded['input_ids'])\n                temp_att_mask.append(encoded['attention_mask'])\n                temp_token_ids.append(encoded['token_type_ids'])\n        \n        input_ids[t_id] = temp_input_ids\n        att_mask[t_id] = temp_att_mask\n        token_ids[t_id] = temp_token_ids\n        \n        #print(input_ids)\n        \n                \n    #print(input_ids)\n    return input_ids, att_mask, token_ids","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jaccard(list1, list2):\n    intersection = len(list(set(list1).intersection(list2)))\n    union = (len(list1) + len(list2)) - intersection\n    return float(intersection) / union","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def similarity(pair):\n    split_id = pair.index(102)\n    #print(split_id)\n    \n    pair_1 = pair[1:split_id]\n    pair_2 = pair[split_id+1:-1]\n    #print(pair_1) \n    #print(pair_2)\n    \n    pair_similarity = jaccard(pair_1, pair_2)\n    #print(pair_similarity)\n    \n    return(pair_similarity)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DATA SETUP FOR INFERENCE ","metadata":{}},{"cell_type":"code","source":"t_ids = df_trial['text_id'].to_numpy()\nclean(df_trial, t_ids)\ndf_trial.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"v_dict = vocab_dictionary(df_trial, t_ids)\nfreq_ratio = frequency_ratio(v_dict, t_ids, maxlen)\n\npos_inputs = get_processing_hash(df_trial, t_ids, 'pos', maxlen)\ntag_inputs = get_processing_hash(df_trial, t_ids, 'tag', maxlen)\ndep_inputs = get_processing_hash(df_trial, t_ids, 'dep', maxlen)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unclean_pairs_ids, unclean_pairs_masks, unclean_pairs_tokens = tokenize_text(df_trial, t_ids, tokenizer, maxlen, True)\nbatch_ids, batch_masks, batch_tokens = tokenize_text(df_trial, t_ids, tokenizer, maxlen, False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unclean_similarity = []\n\nfor t_id in tqdm(t_ids):\n    #print(unclean_pairs_ids[t_id])\n    temp_sim = []\n    i = 0\n    for pair in unclean_pairs_ids[t_id]:\n        if i < maxsent: \n            #print(pair)\n            temp_sim.append(similarity(pair))\n            i += 1\n        else:\n            break\n    \n    temp_sim = equalize_lists(temp_sim, maxsent)\n    unclean_similarity.append(temp_sim)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_ids, batch_masks, batch_tokens = tokenize_text(df_trial, t_ids, tokenizer, maxlen, False)\n\nbert_ids = []\nbert_masks = []\nbert_tokens = []\n\nfor t_id in t_ids:\n    bert_ids.append((batch_ids[t_id][0][0]))\n    bert_masks.append((batch_masks[t_id][0][0]))\n    bert_tokens.append((batch_tokens[t_id][0][0]))\n    \n    #break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_ids_np = np.asarray(bert_ids, dtype=np.float64)\nbert_masks_np = np.asarray(bert_masks, dtype=np.float64)\nbert_tokens_np = np.asarray(bert_tokens, dtype=np.float64)\n\nfreq_ratio_np = np.asarray(freq_ratio)\npos_inputs_np = np.asarray(pos_inputs)\ntag_inputs_np = np.asarray(tag_inputs)\ndep_inputs_np = np.asarray(dep_inputs)\nunclean_similarity_np = np.asarray(unclean_similarity)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODEL SETUP FOR INFERENCE","metadata":{}},{"cell_type":"code","source":"def build_reg_model():\n    backbone = TFBertModel.from_pretrained(BASE_MODEL)\n    \n    input_ids = layers.Input(\n        shape=(maxlen,),\n        dtype=tf.int32,\n        name=\"input_ids\",\n    )\n    \n    attention_mask = layers.Input(\n        shape=(maxlen,),\n        dtype=tf.int32,\n        name=\"attention_mask\",\n    )\n    \n    token_type_ids = layers.Input(\n        shape=(maxlen,),\n        dtype=tf.int32,\n        name=\"token_type_ids\",\n    )\n    \n    bert_out = backbone({\n            \"input_ids\": input_ids,\n            \"token_type_ids\": token_type_ids,\n            \"attention_mask\": attention_mask})\n    \n    similarity = tf.keras.layers.Input((maxsent,), dtype=tf.float32, name='similarity') \n    \n    freq_ratio = tf.keras.layers.Input((maxlen,), dtype=tf.float32, name='freq_ratio') \n    \n    pos = tf.keras.layers.Input((maxlen,), dtype=tf.float32, name='pos') \n    tag = tf.keras.layers.Input((maxlen,), dtype=tf.float32, name='tag') \n    dep = tf.keras.layers.Input((maxlen,), dtype=tf.float32, name='dep') \n    \n    bert_out = tf.keras.layers.Dense(64, activation=\"linear\", dtype=\"float32\")(bert_out[0][:, 0, :])\n    concat = tf.keras.layers.Concatenate()([similarity, freq_ratio, pos, tag, dep, bert_out])\n    \n    x = tf.keras.layers.LayerNormalization()(concat)\n    x = tf.keras.layers.Dense(32)(x)\n    x = tf.keras.layers.LayerNormalization()(x)\n    x = tf.keras.layers.Dense(16)(x)\n    x = tf.keras.layers.LayerNormalization()(x)\n    \n    out = tf.keras.layers.Dense(6, dtype='float32')(x)\n    \n    model = tf.keras.Model(\n        inputs=[input_ids, attention_mask, token_type_ids, similarity, freq_ratio, pos, tag, dep],\n        outputs=[out],\n    )\n    \n    \n    model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n    loss=tf.keras.losses.MeanSquaredError()\n    )\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reloaded_reg_model = build_reg_model()\nreloaded_reg_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reloaded_reg_model.load_weights('../input/ell-model-weights-callback/model_callback.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reg_preds = reloaded_reg_model.predict([bert_ids_np, bert_masks_np, bert_tokens_np, \n                                        unclean_similarity_np, freq_ratio_np, \n                                        pos_inputs_np, tag_inputs_np, dep_inputs_np])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"length = len(t_ids)\n\ncohesion_preds = np.zeros(length)\nsyntax_preds = np.zeros(length)\nvocabulary_preds = np.zeros(length)\nphraseology_preds = np.zeros(length)\ngrammar_preds = np.zeros(length)\nconventions_preds = np.zeros(length)\n\nfor i in range(length):\n    cohesion_preds[i] = reg_preds[i][0]\n    syntax_preds[i] = reg_preds[i][1]\n    vocabulary_preds[i] = reg_preds[i][2]\n    phraseology_preds[i] = reg_preds[i][3]\n    grammar_preds[i] = reg_preds[i][4]\n    conventions_preds[i] = reg_preds[i][5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = pd.DataFrame()\npredictions['text_id'] = t_ids\n\npredictions['cohesion'] = cohesion_preds\npredictions['syntax'] = syntax_preds\npredictions['vocabulary'] = vocabulary_preds\npredictions['phraseology'] = phraseology_preds\npredictions['grammar'] = grammar_preds\npredictions['conventions'] = conventions_preds\n\npredictions.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}