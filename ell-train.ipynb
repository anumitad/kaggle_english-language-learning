{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer\nfrom transformers import BertTokenizer, TFBertModel\nimport re\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nnltk.download('punkt')\n!pip install contractions\nimport contractions\nimport copy \nnltk.download('stopwords')\nnltk.download('omw-1.4')\nfrom textblob import TextBlob\nimport spacy\nfrom spacy import displacy\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import regularizers\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tqdm import tqdm\ntqdm.pandas()\n\nfrom lightgbm import LGBMRegressor\nfrom sklearn import model_selection\nfrom sklearn import metrics","metadata":{"execution":{"iopub.status.busy":"2022-11-14T03:55:24.979556Z","iopub.execute_input":"2022-11-14T03:55:24.980376Z","iopub.status.idle":"2022-11-14T03:56:00.706568Z","shell.execute_reply.started":"2022-11-14T03:55:24.979881Z","shell.execute_reply":"2022-11-14T03:56:00.705474Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2022-11-14 03:55:34.077994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 03:55:34.078936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 03:55:34.080013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 03:55:34.080859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 03:55:34.081618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 03:55:34.082361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 03:55:34.086198: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-11-14 03:55:34.336888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 03:55:34.337788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 03:55:34.338573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 03:55:34.339295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 03:55:34.340003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 03:55:34.340704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 03:55:41.798231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 03:55:41.799137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 03:55:41.799885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 03:55:41.800645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 03:55:41.801396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 03:55:41.802093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13351 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n2022-11-14 03:55:41.806100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-14 03:55:41.806818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13351 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"Collecting contractions\n  Downloading contractions-0.1.72-py2.py3-none-any.whl (8.3 kB)\nCollecting textsearch>=0.0.21\n  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\nCollecting pyahocorasick\n  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting anyascii\n  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.5/287.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\nSuccessfully installed anyascii-0.3.1 contractions-0.1.72 pyahocorasick-1.4.4 textsearch-0.0.24\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"cell_type":"code","source":"BASE_MODEL = '../input/huggingface-bert-variants/bert-base-cased/bert-base-cased'\ntokenizer = BertTokenizer.from_pretrained(BASE_MODEL)\nnlp = spacy.load(\"en_core_web_sm\")\nmaxlen = 100\nmaxsent = 10","metadata":{"execution":{"iopub.status.busy":"2022-11-14T03:56:00.708716Z","iopub.execute_input":"2022-11-14T03:56:00.709108Z","iopub.status.idle":"2022-11-14T03:56:01.487244Z","shell.execute_reply.started":"2022-11-14T03:56:00.709073Z","shell.execute_reply":"2022-11-14T03:56:01.486235Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df_trial = pd.read_csv('../input/ell-clean-text/ELL-clean.csv')\ndf_trial.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-14T03:56:01.488668Z","iopub.execute_input":"2022-11-14T03:56:01.489052Z","iopub.status.idle":"2022-11-14T03:56:01.808038Z","shell.execute_reply.started":"2022-11-14T03:56:01.489014Z","shell.execute_reply":"2022-11-14T03:56:01.806929Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"        text_id                                          full_text  cohesion  \\\n0  0016926B079C  I think that students would benefit from learn...       3.5   \n1  0022683E9EA5  When a problem is a change you have to let it ...       2.5   \n2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0   \n3  003885A45F42  The best time in life is when you become yours...       4.5   \n4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5   \n\n   syntax  vocabulary  phraseology  grammar  conventions  \\\n0     3.5         3.0          3.0      4.0          3.0   \n1     2.5         3.0          2.0      2.0          2.5   \n2     3.5         3.0          3.0      3.0          2.5   \n3     4.5         4.5          4.5      4.0          5.0   \n4     3.0         3.0          3.0      2.5          2.5   \n\n                                          clean_text  \n0  student benefit learning home change early mor...  \n1  problem change matter happening change mind wa...  \n2  dear principal change school policy grade b av...  \n3  time life agree greatest accomplishment world ...  \n4  small act kindness impact change person impact...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>full_text</th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n      <th>clean_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0016926B079C</td>\n      <td>I think that students would benefit from learn...</td>\n      <td>3.5</td>\n      <td>3.5</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>student benefit learning home change early mor...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0022683E9EA5</td>\n      <td>When a problem is a change you have to let it ...</td>\n      <td>2.5</td>\n      <td>2.5</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.5</td>\n      <td>problem change matter happening change mind wa...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00299B378633</td>\n      <td>Dear, Principal\\n\\nIf u change the school poli...</td>\n      <td>3.0</td>\n      <td>3.5</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>2.5</td>\n      <td>dear principal change school policy grade b av...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>003885A45F42</td>\n      <td>The best time in life is when you become yours...</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>time life agree greatest accomplishment world ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0049B1DF5CCC</td>\n      <td>Small act of kindness can impact in other peop...</td>\n      <td>2.5</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>2.5</td>\n      <td>2.5</td>\n      <td>small act kindness impact change person impact...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# HELPER FUNCTIONS","metadata":{}},{"cell_type":"code","source":"def clean(df, t_ids):\n    clean_text = []\n    \n    for t_id in tqdm(t_ids):\n        temp_df = df.loc[df['text_id']==t_id]\n        \n        text = temp_df['full_text'].to_numpy()\n        #print(text)\n    \n        \n        text = text[0].lower()\n        text = re.sub(\"[^\\w\\s]\", \" \", text)\n        \n        text = text.split()\n        \n        \n        text_nostop = []\n        for word in text:\n            if word not in stopwords.words():\n                text_nostop.append(word)\n                \n        lemmatizer = WordNetLemmatizer()\n        text_lem = []\n        for word in text_nostop:\n            text_lem.append(lemmatizer.lemmatize(word))\n            \n        text = \" \".join(text_lem)\n        clean_text.append(text)\n        \n        # break\n        \n    #print(clean_text)\n    df['clean_text'] = clean_text","metadata":{"execution":{"iopub.status.busy":"2022-11-14T03:56:01.811090Z","iopub.execute_input":"2022-11-14T03:56:01.811861Z","iopub.status.idle":"2022-11-14T03:56:01.819412Z","shell.execute_reply.started":"2022-11-14T03:56:01.811821Z","shell.execute_reply":"2022-11-14T03:56:01.818212Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def equalize_lists(lst, maxlen):\n    length = len(lst)\n    dif = abs(maxlen - length)\n    \n    if maxlen > length and dif != 0:\n        lst += [0] * dif\n        \n    elif maxlen < length and dif > 0:\n        lst = lst[:len(lst)-dif]\n        \n    #print(lst)\n    return lst","metadata":{"execution":{"iopub.status.busy":"2022-11-14T03:56:01.821156Z","iopub.execute_input":"2022-11-14T03:56:01.821576Z","iopub.status.idle":"2022-11-14T03:56:01.833728Z","shell.execute_reply.started":"2022-11-14T03:56:01.821512Z","shell.execute_reply":"2022-11-14T03:56:01.832515Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def vocab_dictionary(df, t_ids):\n    vocab_dict = {}\n    \n    for t_id in tqdm(t_ids):\n        temp_df = df.loc[df['text_id']==t_id]\n        temp_dict = {}\n        \n        text = temp_df['clean_text'].to_numpy()\n        words = text[0].split()\n        #print(words)\n        \n        for word in words:\n            if word not in temp_dict.keys():\n                temp_dict[word] = 1\n            \n            else:\n                temp_dict[word] += 1\n         \n        #print()\n        #print(temp_dict)\n        \n        vocab_dict[t_id] = temp_dict\n        #print(vocab_dict)\n        \n        #break\n        \n    return vocab_dict","metadata":{"execution":{"iopub.status.busy":"2022-11-14T03:56:01.835889Z","iopub.execute_input":"2022-11-14T03:56:01.836721Z","iopub.status.idle":"2022-11-14T03:56:01.848022Z","shell.execute_reply.started":"2022-11-14T03:56:01.836685Z","shell.execute_reply":"2022-11-14T03:56:01.847093Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def frequency_ratio(vocab_dict, t_ids, maxlen):\n    #print(vocab_dict)\n    out = []\n    \n    for t_id in tqdm(t_ids):\n        vals = vocab_dict[t_id]\n        #print(vals)\n        freq = list(vals.values())\n        #print(freq)\n        max_use = max(freq)\n        #print(max_use)\n        \n        temp = [val/max_use for val in freq]\n        temp = equalize_lists(temp, maxlen)\n        temp = np.asarray(temp)\n        #print(freq)\n        #print(temp)\n        out.append(temp)\n        \n        #break \n        \n    #print(out)\n    return out ","metadata":{"execution":{"iopub.status.busy":"2022-11-14T03:56:01.849580Z","iopub.execute_input":"2022-11-14T03:56:01.850038Z","iopub.status.idle":"2022-11-14T03:56:01.858818Z","shell.execute_reply.started":"2022-11-14T03:56:01.850005Z","shell.execute_reply":"2022-11-14T03:56:01.857953Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def get_processing_hash(df, t_ids, feature, maxlen):\n    out = []\n    \n    all_text = df['full_text'].to_list()\n    \n    for i, text in enumerate(tqdm(all_text)):\n        #print(i)\n        temp_out = []\n        #print(text)\n        doc = nlp(text)\n        \n        for token in doc:\n            #print(token, token.pos_)\n            try: \n                if feature == 'pos':\n                    temp_out.append(token.pos)\n\n                elif feature == 'tag':\n                    temp_out.append(token.tag)\n\n                else:\n                    temp_out.append(token.dep)\n                    \n            except Exception:\n                pass\n                \n        #print(temp_out)\n        length = len(temp_out)\n        temp_out = equalize_lists(temp_out, maxlen)\n            \n        out.append(temp_out)\n        #break \n        \n    #print(out)\n    return out","metadata":{"execution":{"iopub.status.busy":"2022-11-14T03:56:01.860553Z","iopub.execute_input":"2022-11-14T03:56:01.861068Z","iopub.status.idle":"2022-11-14T03:56:01.871369Z","shell.execute_reply.started":"2022-11-14T03:56:01.861022Z","shell.execute_reply":"2022-11-14T03:56:01.870424Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def tokenize_text(df, t_ids, tokenizer, maxlen, pairwise=False):\n    input_ids = {}\n    att_mask = {}\n    token_ids = {}\n    \n    '''if clean == False: \n        use = 'full_text'\n    else:\n        use = 'clean_text'''\n    \n    for t_id in tqdm(t_ids):\n        temp_df = df.loc[df['text_id']==t_id]\n        \n        temp_input_ids = []\n        temp_att_mask = []\n        temp_token_ids = []\n        \n        text = temp_df['full_text'].to_numpy() # THIS USED TO BE USE \n        #print(text)\n        #sentences = text[0].split()\n        sentences = nltk.tokenize.sent_tokenize(text[0])\n        #print(sentences)\n        \n        if pairwise == False:\n            #for sentence in sentences:\n            encoded = tokenizer.batch_encode_plus(text, add_special_tokens=True, return_token_type_ids=True, truncation=True, \n                                                 padding='max_length', max_length=maxlen)\n\n            temp_input_ids.append(encoded['input_ids'])\n            temp_att_mask.append(encoded['attention_mask'])\n            temp_token_ids.append(encoded['token_type_ids'])\n            \n            #print(encoded['input_ids'])\n            #print(encoded['token_type_ids'])\n                \n        else:\n            #print(\"pointwise\")\n            #print(sentences)\n            for i in range(len(sentences)-1):\n                #print(i)\n                pair = sentences[i:i+2]\n                #print(pair)\n                \n                encoded = tokenizer.encode_plus(pair[0], pair[1], add_special_tokens=True, return_token_type_ids=True, truncation=False,\n                                                padding='max_length', max_length=maxlen)\n                \n                temp_input_ids.append(encoded['input_ids'])\n                temp_att_mask.append(encoded['attention_mask'])\n                temp_token_ids.append(encoded['token_type_ids'])\n        \n        input_ids[t_id] = temp_input_ids\n        att_mask[t_id] = temp_att_mask\n        token_ids[t_id] = temp_token_ids\n        \n        #print(input_ids)\n        \n                \n    #print(input_ids)\n    return input_ids, att_mask, token_ids","metadata":{"execution":{"iopub.status.busy":"2022-11-14T03:56:01.872755Z","iopub.execute_input":"2022-11-14T03:56:01.873161Z","iopub.status.idle":"2022-11-14T03:56:01.887418Z","shell.execute_reply.started":"2022-11-14T03:56:01.873104Z","shell.execute_reply":"2022-11-14T03:56:01.886367Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def jaccard(list1, list2):\n    intersection = len(list(set(list1).intersection(list2)))\n    union = (len(list1) + len(list2)) - intersection\n    return float(intersection) / union","metadata":{"execution":{"iopub.status.busy":"2022-11-14T03:56:01.892272Z","iopub.execute_input":"2022-11-14T03:56:01.892586Z","iopub.status.idle":"2022-11-14T03:56:01.903282Z","shell.execute_reply.started":"2022-11-14T03:56:01.892562Z","shell.execute_reply":"2022-11-14T03:56:01.902278Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def similarity(pair):\n    split_id = pair.index(102)\n    #print(split_id)\n    \n    pair_1 = pair[1:split_id]\n    pair_2 = pair[split_id+1:-1]\n    #print(pair_1) \n    #print(pair_2)\n    \n    pair_similarity = jaccard(pair_1, pair_2)\n    #print(pair_similarity)\n    \n    return(pair_similarity)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T03:56:01.904718Z","iopub.execute_input":"2022-11-14T03:56:01.905260Z","iopub.status.idle":"2022-11-14T03:56:01.914660Z","shell.execute_reply.started":"2022-11-14T03:56:01.905220Z","shell.execute_reply":"2022-11-14T03:56:01.913664Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# PRETRAIN SETUP","metadata":{}},{"cell_type":"code","source":"t_ids = df_trial['text_id'].to_numpy()","metadata":{"execution":{"iopub.status.busy":"2022-11-14T03:56:01.916171Z","iopub.execute_input":"2022-11-14T03:56:01.916543Z","iopub.status.idle":"2022-11-14T03:56:01.927895Z","shell.execute_reply.started":"2022-11-14T03:56:01.916479Z","shell.execute_reply":"2022-11-14T03:56:01.926986Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"v_dict = vocab_dictionary(df_trial, t_ids)\nfreq_ratio = frequency_ratio(v_dict, t_ids, maxlen)\n\npos_inputs = get_processing_hash(df_trial, t_ids, 'pos', maxlen)\ntag_inputs = get_processing_hash(df_trial, t_ids, 'tag', maxlen)\ndep_inputs = get_processing_hash(df_trial, t_ids, 'dep', maxlen)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T03:56:01.929449Z","iopub.execute_input":"2022-11-14T03:56:01.929851Z","iopub.status.idle":"2022-11-14T04:07:53.341207Z","shell.execute_reply.started":"2022-11-14T03:56:01.929818Z","shell.execute_reply":"2022-11-14T04:07:53.340109Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"100%|██████████| 3911/3911 [00:02<00:00, 1326.91it/s]\n100%|██████████| 3911/3911 [00:00<00:00, 57182.83it/s]\n100%|██████████| 3911/3911 [04:02<00:00, 16.14it/s]\n100%|██████████| 3911/3911 [03:53<00:00, 16.73it/s]\n100%|██████████| 3911/3911 [03:52<00:00, 16.83it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"unclean_pairs_ids, unclean_pairs_masks, unclean_pairs_tokens = tokenize_text(df_trial, t_ids, tokenizer, maxlen, True)\nbatch_ids, batch_masks, batch_tokens = tokenize_text(df_trial, t_ids, tokenizer, maxlen, False)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T04:07:53.342593Z","iopub.execute_input":"2022-11-14T04:07:53.343061Z","iopub.status.idle":"2022-11-14T04:09:57.368913Z","shell.execute_reply.started":"2022-11-14T04:07:53.343016Z","shell.execute_reply":"2022-11-14T04:09:57.367961Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"100%|██████████| 3911/3911 [01:22<00:00, 47.33it/s]\n100%|██████████| 3911/3911 [00:41<00:00, 94.53it/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"unclean_similarity = []\n\nfor t_id in tqdm(t_ids):\n    #print(unclean_pairs_ids[t_id])\n    temp_sim = []\n    i = 0\n    for pair in unclean_pairs_ids[t_id]:\n        if i < maxsent: \n            #print(pair)\n            temp_sim.append(similarity(pair))\n            i += 1\n        else:\n            break\n    \n    temp_sim = equalize_lists(temp_sim, maxsent)\n    unclean_similarity.append(temp_sim)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T04:09:57.370490Z","iopub.execute_input":"2022-11-14T04:09:57.370921Z","iopub.status.idle":"2022-11-14T04:09:57.623133Z","shell.execute_reply.started":"2022-11-14T04:09:57.370884Z","shell.execute_reply":"2022-11-14T04:09:57.622078Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"100%|██████████| 3911/3911 [00:00<00:00, 16128.29it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_ids, batch_masks, batch_tokens = tokenize_text(df_trial, t_ids, tokenizer, maxlen, False)\n\nbert_ids = []\nbert_masks = []\nbert_tokens = []\n\nfor t_id in t_ids:\n    bert_ids.append((batch_ids[t_id][0][0]))\n    bert_masks.append((batch_masks[t_id][0][0]))\n    bert_tokens.append((batch_tokens[t_id][0][0]))\n    \n    #break","metadata":{"execution":{"iopub.status.busy":"2022-11-14T04:09:57.624638Z","iopub.execute_input":"2022-11-14T04:09:57.625800Z","iopub.status.idle":"2022-11-14T04:10:38.668618Z","shell.execute_reply.started":"2022-11-14T04:09:57.625759Z","shell.execute_reply":"2022-11-14T04:10:38.667010Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"100%|██████████| 3911/3911 [00:41<00:00, 95.34it/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"cohesion_labels = df_trial['cohesion'].to_numpy()\nsyntax_labels = df_trial['syntax'].to_numpy()\nvocabulary_labels = df_trial['vocabulary'].to_numpy()\nphraseology_labels = df_trial['phraseology'].to_numpy()\ngrammar_labels = df_trial['grammar'].to_numpy()\nconventions_labels = df_trial['conventions'].to_numpy()\n\nbert_ids_np = np.asarray(bert_ids, dtype=np.float64)\nbert_masks_np = np.asarray(bert_masks, dtype=np.float64)\nbert_tokens_np = np.asarray(bert_tokens, dtype=np.float64)\n\nfreq_ratio_np = np.asarray(freq_ratio)\npos_inputs_np = np.asarray(pos_inputs)\ntag_inputs_np = np.asarray(tag_inputs)\ndep_inputs_np = np.asarray(dep_inputs)\nunclean_similarity_np = np.asarray(unclean_similarity)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T04:10:38.670269Z","iopub.execute_input":"2022-11-14T04:10:38.670648Z","iopub.status.idle":"2022-11-14T04:10:38.891757Z","shell.execute_reply.started":"2022-11-14T04:10:38.670611Z","shell.execute_reply":"2022-11-14T04:10:38.890817Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"length = len(t_ids)\nlabels = np.zeros((length, 6))\n\nfor i in range(length):\n    labels[i][0] = cohesion_labels[i]\n    labels[i][1] = syntax_labels[i]\n    labels[i][2] = vocabulary_labels[i]\n    labels[i][3] = phraseology_labels[i]\n    labels[i][4] = grammar_labels[i]\n    labels[i][5] = conventions_labels[i]","metadata":{"execution":{"iopub.status.busy":"2022-11-14T04:10:38.893077Z","iopub.execute_input":"2022-11-14T04:10:38.893543Z","iopub.status.idle":"2022-11-14T04:10:38.908345Z","shell.execute_reply.started":"2022-11-14T04:10:38.893506Z","shell.execute_reply":"2022-11-14T04:10:38.907285Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# BUILD MODEL AND TRAIN","metadata":{}},{"cell_type":"code","source":"def build_reg_model():\n    backbone = TFBertModel.from_pretrained(BASE_MODEL)\n    \n    input_ids = layers.Input(\n        shape=(maxlen,),\n        dtype=tf.int32,\n        name=\"input_ids\",\n    )\n    \n    attention_mask = layers.Input(\n        shape=(maxlen,),\n        dtype=tf.int32,\n        name=\"attention_mask\",\n    )\n    \n    token_type_ids = layers.Input(\n        shape=(maxlen,),\n        dtype=tf.int32,\n        name=\"token_type_ids\",\n    )\n    \n    bert_out = backbone({\n            \"input_ids\": input_ids,\n            \"token_type_ids\": token_type_ids,\n            \"attention_mask\": attention_mask})\n    \n    similarity = tf.keras.layers.Input((maxsent,), dtype=tf.float32, name='similarity') \n    \n    freq_ratio = tf.keras.layers.Input((maxlen,), dtype=tf.float32, name='freq_ratio') \n    \n    pos = tf.keras.layers.Input((maxlen,), dtype=tf.float32, name='pos') \n    tag = tf.keras.layers.Input((maxlen,), dtype=tf.float32, name='tag') \n    dep = tf.keras.layers.Input((maxlen,), dtype=tf.float32, name='dep') \n    \n    bert_out = tf.keras.layers.Dense(64, activation=\"linear\", dtype=\"float32\")(bert_out[0][:, 0, :])\n    concat = tf.keras.layers.Concatenate()([similarity, freq_ratio, pos, tag, dep, bert_out])\n    \n    x = tf.keras.layers.LayerNormalization()(concat)\n    x = tf.keras.layers.Dense(32)(x)\n    x = tf.keras.layers.LayerNormalization()(x)\n    x = tf.keras.layers.Dense(16)(x)\n    x = tf.keras.layers.LayerNormalization()(x)\n    \n    out = tf.keras.layers.Dense(6, dtype='float32')(x)\n    \n    model = tf.keras.Model(\n        inputs=[input_ids, attention_mask, token_type_ids, similarity, freq_ratio, pos, tag, dep],\n        outputs=[out],\n    )\n    \n    \n    model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n    loss=tf.keras.losses.MeanSquaredError()\n    )\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-11-14T04:10:38.958518Z","iopub.execute_input":"2022-11-14T04:10:38.958874Z","iopub.status.idle":"2022-11-14T04:10:38.973276Z","shell.execute_reply.started":"2022-11-14T04:10:38.958837Z","shell.execute_reply":"2022-11-14T04:10:38.972189Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"regmodel = build_reg_model()\nregmodel.summary()\n#plot_model(model,show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T04:10:38.974597Z","iopub.execute_input":"2022-11-14T04:10:38.975091Z","iopub.status.idle":"2022-11-14T04:10:56.244310Z","shell.execute_reply.started":"2022-11-14T04:10:38.975055Z","shell.execute_reply":"2022-11-14T04:10:56.243250Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"Some layers from the model checkpoint at ../input/huggingface-bert-variants/bert-base-cased/bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at ../input/huggingface-bert-variants/bert-base-cased/bert-base-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nattention_mask (InputLayer)     [(None, 100)]        0                                            \n__________________________________________________________________________________________________\ninput_ids (InputLayer)          [(None, 100)]        0                                            \n__________________________________________________________________________________________________\ntoken_type_ids (InputLayer)     [(None, 100)]        0                                            \n__________________________________________________________________________________________________\ntf_bert_model (TFBertModel)     TFBaseModelOutputWit 108310272   attention_mask[0][0]             \n                                                                 input_ids[0][0]                  \n                                                                 token_type_ids[0][0]             \n__________________________________________________________________________________________________\ntf.__operators__.getitem (Slici (None, 768)          0           tf_bert_model[0][0]              \n__________________________________________________________________________________________________\nsimilarity (InputLayer)         [(None, 10)]         0                                            \n__________________________________________________________________________________________________\nfreq_ratio (InputLayer)         [(None, 100)]        0                                            \n__________________________________________________________________________________________________\npos (InputLayer)                [(None, 100)]        0                                            \n__________________________________________________________________________________________________\ntag (InputLayer)                [(None, 100)]        0                                            \n__________________________________________________________________________________________________\ndep (InputLayer)                [(None, 100)]        0                                            \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 64)           49216       tf.__operators__.getitem[0][0]   \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 474)          0           similarity[0][0]                 \n                                                                 freq_ratio[0][0]                 \n                                                                 pos[0][0]                        \n                                                                 tag[0][0]                        \n                                                                 dep[0][0]                        \n                                                                 dense[0][0]                      \n__________________________________________________________________________________________________\nlayer_normalization (LayerNorma (None, 474)          948         concatenate[0][0]                \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 32)           15200       layer_normalization[0][0]        \n__________________________________________________________________________________________________\nlayer_normalization_1 (LayerNor (None, 32)           64          dense_1[0][0]                    \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 16)           528         layer_normalization_1[0][0]      \n__________________________________________________________________________________________________\nlayer_normalization_2 (LayerNor (None, 16)           32          dense_2[0][0]                    \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 6)            102         layer_normalization_2[0][0]      \n==================================================================================================\nTotal params: 108,376,362\nTrainable params: 108,376,362\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T04:27:19.521830Z","iopub.execute_input":"2022-11-14T04:27:19.522509Z","iopub.status.idle":"2022-11-14T04:27:19.527414Z","shell.execute_reply.started":"2022-11-14T04:27:19.522472Z","shell.execute_reply":"2022-11-14T04:27:19.526395Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"reghistory = regmodel.fit(x=[bert_ids_np, bert_masks_np, bert_tokens_np, unclean_similarity_np, freq_ratio_np, pos_inputs_np, tag_inputs_np, dep_inputs_np], \n                   y=[labels], validation_split=0.3, batch_size=32, \n                          epochs=100, verbose =1, callbacks=[callback])","metadata":{"execution":{"iopub.status.busy":"2022-11-14T04:27:36.913387Z","iopub.execute_input":"2022-11-14T04:27:36.914394Z","iopub.status.idle":"2022-11-14T04:45:41.447577Z","shell.execute_reply.started":"2022-11-14T04:27:36.914344Z","shell.execute_reply":"2022-11-14T04:45:41.446560Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Epoch 1/100\n86/86 [==============================] - 61s 708ms/step - loss: 0.4806 - val_loss: 0.4476\nEpoch 2/100\n86/86 [==============================] - 60s 698ms/step - loss: 0.4684 - val_loss: 0.4378\nEpoch 3/100\n86/86 [==============================] - 60s 701ms/step - loss: 0.4595 - val_loss: 0.4309\nEpoch 4/100\n86/86 [==============================] - 60s 701ms/step - loss: 0.4522 - val_loss: 0.4294\nEpoch 5/100\n86/86 [==============================] - 60s 703ms/step - loss: 0.4477 - val_loss: 0.4227\nEpoch 6/100\n86/86 [==============================] - 60s 702ms/step - loss: 0.4433 - val_loss: 0.4209\nEpoch 7/100\n86/86 [==============================] - 60s 702ms/step - loss: 0.4405 - val_loss: 0.4192\nEpoch 8/100\n86/86 [==============================] - 60s 701ms/step - loss: 0.4382 - val_loss: 0.4209\nEpoch 9/100\n86/86 [==============================] - 60s 701ms/step - loss: 0.4377 - val_loss: 0.4182\nEpoch 10/100\n86/86 [==============================] - 60s 701ms/step - loss: 0.4355 - val_loss: 0.4166\nEpoch 11/100\n86/86 [==============================] - 60s 702ms/step - loss: 0.4352 - val_loss: 0.4147\nEpoch 12/100\n86/86 [==============================] - 60s 700ms/step - loss: 0.4346 - val_loss: 0.4170\nEpoch 13/100\n86/86 [==============================] - 60s 702ms/step - loss: 0.4337 - val_loss: 0.4147\nEpoch 14/100\n86/86 [==============================] - 60s 702ms/step - loss: 0.4352 - val_loss: 0.4188\nEpoch 15/100\n86/86 [==============================] - 60s 701ms/step - loss: 0.4343 - val_loss: 0.4207\nEpoch 16/100\n86/86 [==============================] - 60s 701ms/step - loss: 0.4340 - val_loss: 0.4173\nEpoch 17/100\n86/86 [==============================] - 60s 702ms/step - loss: 0.4344 - val_loss: 0.4194\nEpoch 18/100\n86/86 [==============================] - 60s 701ms/step - loss: 0.4343 - val_loss: 0.4151\n","output_type":"stream"}]},{"cell_type":"code","source":"regmodel.save_weights(f\"model_callback.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-11-14T04:53:12.811776Z","iopub.execute_input":"2022-11-14T04:53:12.812153Z","iopub.status.idle":"2022-11-14T04:53:13.535150Z","shell.execute_reply.started":"2022-11-14T04:53:12.812120Z","shell.execute_reply":"2022-11-14T04:53:13.534190Z"},"trusted":true},"execution_count":27,"outputs":[]}]}